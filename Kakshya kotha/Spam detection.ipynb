{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559970c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud \n",
    "from collections import Counter\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb2cca8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseError",
     "evalue": "no element found: line 1, column 0 (<string>)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3553\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[2], line 2\u001b[0m\n    nltk.download('punkt')\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\site-packages\\nltk\\downloader.py:777\u001b[0m in \u001b[0;35mdownload\u001b[0m\n    for msg in self.incr_download(info_or_id, download_dir, force):\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\site-packages\\nltk\\downloader.py:629\u001b[0m in \u001b[0;35mincr_download\u001b[0m\n    info = self._info_or_id(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\site-packages\\nltk\\downloader.py:603\u001b[0m in \u001b[0;35m_info_or_id\u001b[0m\n    return self.info(info_or_id)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\site-packages\\nltk\\downloader.py:1009\u001b[0m in \u001b[0;35minfo\u001b[0m\n    self._update_index()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\site-packages\\nltk\\downloader.py:952\u001b[0m in \u001b[0;35m_update_index\u001b[0m\n    ElementTree.parse(urlopen(self._url)).getroot()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mD:\\Anaconda\\Lib\\xml\\etree\\ElementTree.py:1218\u001b[0m in \u001b[0;35mparse\u001b[0m\n    tree.parse(source, parser)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mD:\\Anaconda\\Lib\\xml\\etree\\ElementTree.py:580\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    self._root = parser._parse_whole(source)\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<string>\u001b[1;36m\u001b[0m\n\u001b[1;31mParseError\u001b[0m\u001b[1;31m:\u001b[0m no element found: line 1, column 0\n"
     ]
    }
   ],
   "source": [
    "# Download necessary nltk data\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567a02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/siddhantbhattarai/AI-DataScience-BootCamp/main/SMSSpamCollection'\n",
    "df = pd.read_csv(url, sep='\\t', names=['label', 'message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6135a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c0166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c752106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5ae52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ce232",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f519e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics statics of datasets\n",
    "clean_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3956906",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdc28b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the spam vs ham messages\n",
    "clean_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a290ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Distributing the spam\n",
    "sns.countplot(x='label', data=clean_df)\n",
    "plt.title('Distribution of spam vs ham message')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e810138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the lenght of message\n",
    "clean_df['message_length'] = clean_df['message'].apply(len)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(clean_df[clean_df['label'] == 'ham']['message_length'], label='Ham', color='blue', bins=50, kde=True)\n",
    "sns.histplot(clean_df[clean_df['label'] == 'spam']['message_length'], label='Spam', color='red', bins=50, kde=True)\n",
    "plt.title('Message Length Distribution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24545fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Word clouds for spam and ham messages\n",
    "spam_words = ' '.join(clean_df[clean_df['label'] == 'spam']['message'])\n",
    "ham_words = ' '.join(clean_df[clean_df['label'] == 'ham']['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a10437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_wordCloud = WordCloud(width=800, height=400, background_color='white').generate(spam_words)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(spam_wordCloud, interpolation='bilinear')\n",
    "plt.title('Spam Mesages Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1cee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_wordCloud = WordCloud(width=800, height=400, background_color='white').generate(ham_words)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(ham_wordCloud, interpolation='bilinear')\n",
    "plt.title('Ham Mesages Word Cloud')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b5888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Tokenization and common word analysis\n",
    "clean_df['tokens'] = clean_df['message'].apply(word_tokenize)\n",
    "spam_tokens = [token for sublist in clean_df[clean_df['label'] == 'spam']['tokens'] for token in sublist]\n",
    "ham_tokens = [token for sublist in clean_df[clean_df['label'] == 'ham']['tokens'] for token in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3097e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_common_words = Counter(spam_tokens).most_common(20)\n",
    "ham_common_words = Counter(ham_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9392a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_common_df = pd.DataFrame(spam_common_words, columns=['word', 'count'])\n",
    "ham_common_df = pd.DataFrame(ham_common_words, columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014ca79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='count', y='word', data = spam_common_df, color='red')\n",
    "plt.title('Most Common words in Spam Message')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a96219",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='count', y='word', data = ham_common_df, color='red')\n",
    "plt.title('Most Common words in Ham Message')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc21807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '. join(text.split())\n",
    "    return text\n",
    "clean_df['message'] = clean_df['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cd8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "X = clean_df['message']\n",
    "y = clean_df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230c30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48b52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering: Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9414f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection and Training\n",
    "# Multinomial Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b122ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "svc_model = LinearSVC(dual=False)\n",
    "svc_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "svc_predictions = svc_model.predict(X_test_tfidf)\n",
    "rf_predictions = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Accuracy computation\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "svc_accuracy = accuracy_score(y_test, svc_predictions)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "\n",
    "# Print the accuracy\n",
    "print(f'Multinomial Naive Bayes Accuracy: {nb_accuracy*100:.2f}%')\n",
    "print(f'Support Vector Classifier Accuracy: {svc_accuracy*100:.2f}%')\n",
    "print(f'Random Forest Classifier Accuracy: {rf_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b34892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
